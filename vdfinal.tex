\documentclass{report}

\usepackage{xeCJK}
% \usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amssymb}
\usepackage[top=1.5in, bottom=1.5in, left=1.5in, right=1.5in]{geometry}

\title{Final Report: A Multimedia Information Retrieval Database System}
\author{Liuyang Wang \texttt{lw2635}, Zhixuan Wang \texttt{zw2335}, Zihang Chen \texttt{zc2324}}

\begin{document}
\maketitle
\tableofcontents

\chapter{Overview}

In this document, we would like to elaborate on a multimedia information retrieval database system which retrieves related visual/acoustic information according to the style of multimedia or the users' psychological perceptions about these multimedia objects.

Typical visual database requires users to be familiar with the query system of their own, i.e., they have their own language for query. However, in this system we proposed, users will query in the database with an audio file (e.g. MP3 or OGG) as input for video clips that fits the input than other. Users can also do the query vice versa. Then the system will use various visual/acoustic features and some interconnections between them, along with a machine learning process, to try to identify queried subjects. More on how such retrieval is done will be explained later in this proposal.

When it comes to database population, one way is to evaluate the style of videos and audio and to score each of the objects according to multiple criteria, such as ``adorable'', or ``psychedelic''. The scores will be in a range of $0.0$ to $1.0$. In other words, each object will receive a vector of scores. A machine learning process will be used to fit these scores to the extracted features.

We will discuss the evalution of the system in Chapter \ref{ch:eval}.

\section{Typical Use Cases}

\begin{itemize}
\item Given a piece of music, find in the database the video clips that best fit the music in the aspects of, for instance, rhythm, the emotions expressed, etc. We regard this video as a good example: \url{https://www.youtube.com/watch?v=hBVMVEsyJEE} (The title means ``Goodbye, Heaven''.) In other words, we think that if someone uses the music in this video as input, an ideal query result would be this video clip.
\item Given a video clip, find in the database the pieces of music that best fit the video clip in the aspects of, for instance, emotions/feelings, style, etc. This is exactly the opposite query regarding the first use case. Here we give another good example of such ``fit'': \url{https://www.youtube.com/watch?v=jN-snGdlzSI}.
\item Given a vector of scores, find in the database the related music and video clips that fit the score vector.
\end{itemize}

We consider the above two example videos are good fits to their respective audios because they both are merely animations with artificial shapes and simple colors that can be easily detected and obvious video structures that matches the rhythm and melody of the respective music. We consider this video \url{http://www.nicovideo.jp/watch/sm16068939} (login required, or watch it here \url{https://www.youtube.com/watch?v=2iQKFNYofD0}) to be not so good an example of what the system can query, since it is actually a tiny plotted movie that mostly relates to the lyrics of the song. It is impossible to see if this video clip is a good fit for certain piece of music without actually understanding what is happening in it.

\section{Limitations}

Since this project is only a preliminary demonstration on the combination of computer vision/audition, machine learning and database systems, we won't use many sophisticated visual/acoustic features, nor will we try to implement our system in such a way that it really ``understands'' all the multimedia information in the way human beings do. We want the system to be as simple as possible, yet quite extensible to be able to easily add new ability to it. More on the approach we will be using is discussed in the next section.

\chapter{Approaches and Methods}

\section{Distribution of Workloads}

Because it's a rather complicated project concerning computer vision and audition, machine learning and user experience, we would like to distribute these tasks in a modularized manner, such that a single person concentrates on one of these tasks. We all think that it's best for the project if one of us solely works on feature extraction part, another solely works on machine learning, and the last guy takes care of the rest of the project, i.e. the user interface and testing. If there were only two members of us, we would probably propose to do much less, for example, only query videos with classified tags.

The system will be implemented in the newest version of Python (Python 3.5.1) for fast prototyping, and roughly divided into 3 parts:
\begin{itemize}
\item The \textbf{infrastructure part}: this includes the client-server, web-based frontend that takes inputs from users and display the results; and the backend that maintains a database and invokes other modules as needed. (Zihang Chen) (HTML5, React.js, Pyramid, SQLite with SQLAlchemy as database driver)
\item The \textbf{computer vision and audition part}: this part will concentrate on the functions that extracts various features from video clips or audio files. (Liuyang Wang) (OpenCV, SimpleCV and librosa, etc.)
\item The \textbf{machine learning part}: this includes the module that fits the scores (or classification) to the features. (Zhixuan Wang) (scipy, sklearn, etc.)
\end{itemize}

\section{System Workflow}

The general workflows are depicted as follows\footnote{These two figures have been updated since the prototype demo to reflect the changes we made in the workflows implemented in the system.}:

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{vdf-query.pdf}
  \caption{Typical query workflow}
  \label{fig:qwf}
\end{figure}

The above figure depicts that users are allowed to query using audio/video files or score vectors. When the user queries the database using audio or video clips, their features were extracted and put into the Convoluted Neural Network to get the predicted score vectors, which then were compared against the scores from users using the formula:

\begin{equation}
\label{eq:confidence}
confidence = 1 - {\sum_{i=1}^{8} (P_i - Q_i)^2 \over 8}
\end{equation}

The user can also find clips directly by supplying scores, in which case the system will traverse the database and find the top 20 multimedia clips ordered by descending confidence.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{vdf-pop.pdf}
  \caption{Typical database population workflow}
  \label{fig:pwf}
\end{figure}

Because of the properties of the machine learning part, i.e. they must be trained again after some new objects are submit by user. Thus, the system works best when there are many queries to the database, rather than updates.

\subsection{Development Environment}

The system is developed by three team members with three different development environment (Windows, Ubuntu and one Mac OS X), which makes it difficult for us to synchronize the development environment, especially that we experienced some difficulties installing OpenCV and it’s Python3 package on Windows. Overall, the development went smoothly.

The development environment is:
\begin{itemize}
  \item Machine: 2 high-end Windows laptops, 1 13’’ Macbook Pro
  \item Operating Systems: Windows 10, Ubuntu and Mac OS X
  \item Primarily involved software: Python 3.5, NumPy 1.10, OpenCV 3.1.0, Caffe, librosa 0.4.2, Pyramid 1.6.1, React.js, etc. (secondary dependencies not listed)
  \item Additionally involved hardware: none
  \item IDE: PyCharm 2016.01, WebStorm 2016.01, Visual Studio 2015
\end{itemize}

\section{System Architecture and Implementation}

The system is implemented as a Browser-Server (B/S) application, utilizing the ubiquitousness and power of browser displays and the flexibility of RESTful API. The backend (server) of the system receives RESTful API requests and invokes API of related module.

\subsection{RESTful API}

Then system currently supports the following RESTful APIs:
\begin{description}
\item[Heterogeneous Query] Queries video clips with audio clips and vice versa. When dealing with this kind of requests, the system invokes feature extraction functions on the received query subject (a file object), and uses the machine learning model to predict it's score vector.

After procuring the score vector, the system traverses the database, calculates and sorts the database objects by formula \ref{eq:confidence} between the score vector of the subject and the ones of the objects. Finally it replies with the sorted result set to the frontend which then displays the result set in a grid.

\item[Homogeneous Query] Roughly the same as heterogeneous query, only that the system allows user to query objects using subjects of the same type.

\item[Score Vector Query] Posts a score vector to the server to query something that relates to the posted score vector closely.

\item[Upload] Allows the user to score and upload multiple multimedia files. The system will train (or refine) the machine learning module automatically. Because extracted features are saved in the database, so if the user uploads a clip with the title-artist pair already in the database, the features won't be extracted again, but the score vector be averaged and the machine learning module be refined.

\end{description}


\subsection{Graphic User Interface}

The user interface is a browser-side standalone \textit{React.js} application. It is different from traditional frontend application in that it leverages the full power of Javascript\footnote{Although the source code of the frontend application is written in CoffeeScript.} to manipulate DOM node such that everything the user sees is rendered at client-side. This relieves the backend server of rendering complicated static HTML and wasting the computational power of the server machine.

The graphic user interface of the system is divided into two views. The first one is the Query View, where the user queries for multimedia clips with the file he/she dragged into the dropzone, or the score vector he/she proposed. The result of such query is displayed in a grid, sorted by descending confidence. The header of the grid items will change color according to the confidence: green for $confidence \geqslant 0.667$, orange for $0.667 > confidence \geqslant 0.333$ and gray for $0.333 > confidence$.

The second view is the Upload View, where the user drags multiple multimeida clips into the dropzone and scores each of the clips according to 8 criteria (which are \textit{Psychedelic}, \textit{Vibrant}, \textit{Happy}, \textit{Adorable}, \textit{Gloomy}, \textit{Energetic}, \textit{Romantic}, and \textit{Violent}). When the user hits \textit{Submit}, all these information will go to the server and be saved into database.

\section{Feature Extraction Module}

\subsection{Acoustic Features}

To integrate audio into the machine learning part, audio clips should be represented in a way that can reveal the style or emotion of the audio clips. 

First, there are mainly two ways to represent the raw audio signal, the first way is using the raw sound wave, and the second way is divide audio signal to several frequencies and using the volume of each frequencies at each frame to depict the signal. Both ways can be easily integrated to neural network model for data training and understanding, but they are hard to be integrated into statistics machine learning method without any preprocessing. Using the whole raw data for data training will emphasize the wave pattern for data representation and retrieval, which is potentially a good approach to represent data but it may result in a relatively poor performance for data training and retrieval even when using the popular CNN method.

The feature now we are using to represent audio file the freqenucy samples sampled from audio clips with a fixed sample rate. We use the Fast Fourier Transform algorithm to extract frequecy from audio. The window size of the Fast Fourier Transform algorithm is set as two times as the frames between conjunct samples. The frequency information extracted then will be pack in to several frequency bin at each sample. The range of frequency bin is set refering to the research about human perception of audio frequency, and frequency which humans has most similar perception is packed within one bin. For each audio, we extract feature from the begining in a fixed length of the audio to guarantee that each feature has the same dimension. The feature extracted can show both the feature in time and frequency domain. We can expect this feature to provide sufficient information for the machine learning part. 

Audio feature extraction is done with Librosa, which provides audio load and FFT extraction function.

\subsection{Visual Features Extraction Implementation}

In order to distinguish different styles of videos, we have to extract the most significant features of the videos which could best represent the style of the video. It's hard to directly find a feature describing the whole video since both spatial information and temporal information are involved. Therefore we decided to extract spatial information and temporal information separately. Firstly, we divided the whole video into some segments. Within each segment, we just care about the static image information of the segment. We regarded the segment as a set of images and extracted features from it. After we extracted features of all segments, they were arranged in chronological order and regarded as the feature of the whole video.

The problem was to find a good feature to represent the video segment including a set of images. Some widely used features are descriptors of local salient points or blobs such as SIFT which represents local information well and is applied to many computer vision issues. For example, it could be used for recognizing a specific object in the image. However, Our purpose is to classify videos according to their styles. So we care more about what feelings the whole video segment could give us. Local feature descriptor seems not to be a good choice. we agreed that the color was an important information to express feelings and decided to use the color histogram as the descriptor of the video.

We implemented three different features based on the color histogram. The first one was RGB color histogram. In RGB color space, each pixel of image is represented as a 3-D vector $(r, g, b)$.For each frame $f$ of the segment, we calculated the quantized RGB color histogram ${\bf{h}_f} = [{h_{1\times1\times1}},...,{h_{N\times N\times N}}]$. For example, if the step number $N$ of each channel was 5, ${\bf{h}}$ would have a 125 dimensional vector. After that the histogram was normalized to satisfy $\sum({h_i}) = 1$.  Since a segment consisted of several frames, we applied a average operator to pool all histogram into a single vector ${\bf{h}_s} = avg(\bf{h}_f)$. Fig.\ref{fig:histo} shows this process. To speed up the feature extraction module, we did not calculate the histogram for every frame in the segment. The number of calculated frames was set according to the consideration of time cost. After the histogram feature ${\bf{h}_s}$ of all segments was generated, they were concatenated together to represent the whole video.

\begin{figure*}[htb]
\centering
\includegraphics[width=0.6\textwidth]{show.eps}
\caption{The process of generating histogram feature of a video segment.}
\label{fig:histo}
\end{figure*}

The second method was HSV color histogram method. It was similar to RGB color histogram method. Because HSV color space is more intuitive and perceptually than RGB color space, we believed the histogram in HSV space was better to realize our objection. Different from RGB method which the three channels were quantized equally, hue was a more significant variable here. To quantize the colors, we used more hues to quantize than saturations and values. In our project, we set $(h, s, v) = (18, 3, 3)$. Finally, we also got a histogram vector ${\bf{h}_s}$ to represent a segment and concatenated all vectors to represent the video.

After generating the color histogram, an important information would be discarded if we regarded the histogram as a vector directly. It was the similarity between different quantized colors. Different dimensions of the histogram vector are related to each other and should not be analyze separately in Machine Learning Module. For example, orange is more similar to red than blue. the similarity between any two colors $c_i = (h_i, s_i, v_i)$ and $c_j = (h_j, s_j, v_j)$, is given by \[{a_{i,j}} = 1 - {\raise0.7ex\hbox{$1$} \!\mathord{\left/
 {\vphantom {1 {\sqrt 5 }}}\right.\kern-\nulldelimiterspace}
\!\lower0.7ex\hbox{${\sqrt 5 }$}}{\left[ {{{({v_i} - {v_j})}^2} + {{({s_i}\cos {h_i} - {s_j}\cos {h_j})}^2} + {{({s_i}\sin {h_i} - {s_j}\sin {h_j})}^2}} \right]^{1/2}}\]
According to this metric, we introduced Histogram Quadratic Distance compare different histogram vector and implement a soft coding method to code histogram vectors as the final feature of a video segment.

First of all, we implemented a clustering algorithm on thousands of HSV histogram vectors ${\bf{h}_1, ...,  \bf{h}_N}$. These histogram vectors are extracted from all existed videos in the database. Since Histogram Quadratic Distance is different from Euclidean distance, instead of k-means algorithm, we used k-medoids algorithm to find the centers of different histogram clusters $\{\bf{h}_{c1},...,\bf{h}_{ck}\}$. In the project we set $k = 64$.

Then for every color histogram vector ${\bf{h}_s}$ extracted from the segment, we calculated the Histogram Quadratic Distance $d^{hist}_{c,s} = (\bf{h}_s - \bf{h}_c)^t\bf{A}(\bf{h}_s - \bf{h}_c)$ as described in VisualSEEK paper bewteen ${\bf{h}_f}$ and each $\bf{h}_c$ and generated the final feature descriptor as $\bf{\mu}=[\mu_1,...,\mu_k]$, where $\mu_i$ could be calculated by
\[{\mu _{\rm{i}}} = \frac{{{e^{ - \left( {\beta  \cdot d(f,{c_i})} \right)}}}}{{\sum\limits_{j = 1}^k {{e^{ - \left( {\beta  \cdot d(f,{c_j})} \right)}}} }}\].
In this way, ${\bf{h}_s}$ was represented by its relationship $\bf{\mu}$ to $\{\bf{c}_1,...,\bf{c}_k\}$. $\bf{\mu}$ was the final descriptor of the video segment. Fig.\ref{fig:coding} shows this process.

\begin{figure*}[htb]
\centering
\includegraphics[width=0.6\textwidth]{coding.eps}
\caption{Soft Coding Feature.}
\label{fig:coding}
\end{figure*}

The methods were written in Python3.5 with OpenCV3.1.0. The input parameters are the video's filename, time length to analyze, number of segments and some other parameters as we described. It took about 3 seconds to analyze a 480x360 2-minute video.

\section{Machine Learning Module}

In this project, we want to construct a database that can integrate audio/video clips and plaintext tags together. To build a database system that can do content-based data retrieval, we have to introduce some artificial intelligence approaches for data mining and retrieval.

To implement the data retrieval part we are going to use some exemplary well-annotated data to train the machine learning module of the database. And then generalize the data using trained machine learning module and store the result in database. When the user input comes, the database first computes the representation of the data (e.g., the scores we talked about in previous sections) via the trained machine learning module, and then find the most similar data in the database.

First, the most basic problem is that how to measure the similarity between the information with such different nature. Fortunately, the media files we can obtain often consist of both video and audio parts and even with some tag description, which will naturally provide us with the correlation information between video, audio and tags. Some websites also provide tags and categories for videos, which can be introduced into our machine learning module as training dataset.
 
We designed and implemented 2 different learning model for the problem.

\subsection{Cluster Classifier Model}

This model learns from the score information we have annotated to the video and audio clips.

First we cluster those score information into several clusters with clustering algorithm. Then we know which cluster each audio file or video file belongs to. After this, we classify the audio or video file to the cluster they belong to and train their features with the classification machine. We can get a classification machine which generates the probability of a video or audio clip belonging to each of the clusters with the extracted features of the files as the input. With the classification machine, we reduce each feature to a probability vector of a small demension. To compare the distance of the video and audio clips, we can compare their generated probability vector. 

As for the complexity of the algorithm, we concluded that the complexity for the training part is large. The computational efforts are mainly done to train the classification machine. We can expect that the training process to take $O(size_{dataset} \times dimension_{feature} \times times_{iteration})$.

Generally speaking, the feature demension should be several thousands and the iteration times be over 100 to achieve a good convergence of the classification machine. But this complexity is acceptable because the training process is done offline. With the classification machine trained, we can get the probability vector in linear time with respect to the dimension of the feature.

We have implemented this model using GMM as the clustering algorithm and the Logistic Regression algorithm as the classification machine using the \textit{sklearn} library.

Though this model is quite straight forward, it seems to work well with the given dataset. We used this model to retrieve files from the database, then relevance result give by this model is quite reasonable in our perception. We will test accurate performance of this algorithm later.

This model has a drawback that we can only return similar video and audio clips, but fail to score the given data automatically with the trained classifier. The second algorithm described below doesn’t have such drawback.

\subsection{CNN regression from feature to score}

In the prototype we mentioned that we were trying to use a regression model to map features to scores. After some dicussions and tests, we found out that traditional statistical method can hardly make a good regression of this problem. So we decide to use CNN to do the non-linear regression.

CNN is a multi-layer model which maps the input features to different features at each layer. The convolution and pooling can efficiently utilize the locality structure of the features, which is suitable for the histogram per frame features we are using. The convolution and pooling layer can abstract the features within some frames. And because the parameters are shared within all neurons of the same layer, the CNN model can extract features that preserve time offset invariant and temporal sequence information within that two layers. The inner product layers at the end of CNN pipe line are fully connected layers which abstract all features extracted by convolution and pooling layers, and finally map the features to scores. So the CNN model seems to satisfy the problem well.

For the implementation of our CNN module, we used a leNet MNIST model to train the features. Features of different fequency bins (for audio) or different color histogram bins (for video) are treated as different channels in the CNN training model. And features are convoluted and pooled in temporal axis. For the loss calculation for CNN, we use Euclidean distance between the last inner production layer and the scores, so the output of the last inner production layer will be the prediction for the video or the audio clip.

The CNN pipeline can be illustrate as: data $\rightarrow$ convolution $\rightarrow$ pooling $\rightarrow$ convolution $\rightarrow$ pooling $\rightarrow$ inner production $\rightarrow$ inner production $\rightarrow$ loss.

This pipeline is implemented using Caffe with GPU acceleration. From our experiment, the CNN model will converge after 2000 iterations, so the training process can be done extremely fast with GPU acceleration.


\chapter{Performance Measurements}
\label{ch:eval}

\section{Database Population}

We have manually selected 40 2-minute video clips with audio from YouTube and stored them in local storage. We then used \texttt{ffmpeg} to convert the video clips to audio clips, thus we now have 40 video clips and 40 audio clips. After this step, we randomly selected 20 video and audio clips\footnote{The audio clips are not necessarily from the same video clips.} (40 in total) to be put in the database. The other half to be queried by.

The video and audio clips to be put into the database are annotated manually by two of the team members with one score vector each before actually put in the database. One of the team members concentrates on annotating the audio clips, while the other scores only the video clips.

We provide here two tables on the genres and the artists of the selected music:

\begin{table}[H]
\centering

\label{my-label}
\begin{tabular}{llll}
\hline
\textbf{Artist} & \textbf{Count} & \textbf{Artist} & \textbf{Count} \\ \hline
猫叉Master+ & 3 & DJ SWAMI & 1 \\
Qrispy Joybox & 1 & REDALiCE & 1 \\
NAOKI & 1 & DJ Mass MAD Izm- & 4 \\
Ryu☆ & 5 & D.J.SETUP & 2 \\
Maozon & 1 & USAO & 2 \\
DJ Noriken & 1 & OutPhase & 2 \\
DJ Yoshitaka & 1 & L.E.D. & 2 \\
dj TAKA & 3 & Nhato & 1 \\
Dirty Androids & 2 & Expander & 1 \\
P-Light & 3 & kors k & 3 \\ \hline
\textit{Average} & \textit{2.0} & \textit{Total Artists} & \textit{20}
\end{tabular}
\caption{Artists involved in the database}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{llllll}
\hline
\textbf{Genre} & \textbf{Count} & \textbf{Genre} & \textbf{Count} & \textbf{Genre} & \textbf{Count} \\ \hline
Samba & 3 & Dance & 2 & J-Core & 2 \\
Techno & 5 & Hardcore & 4 & Breakbeats & 2 \\
Rave & 2 & Drumstep & 1 & Pumpcore & 1 \\
Trance & 6 & Pop & 2 & Handz up & 1 \\
Disco & 1 & Full-on & 2 & Glitchstep & 1 \\
Jangle & 1 & Electro & 1 & Nu-NRG & 1 \\ \cline{5-6} 
Dubstep & 1 & World/Electronica & 1 & \textit{Average} & \textit{2.0}
\end{tabular}
\caption{Genres involved in the database}
\end{table}

For the sake of simplicity, secondary genre names like \textit{Speed}, \textit{Happy}, etc. are omitted.

\section{Evaluation}

\subsection{A Brief Analysis on the Performance of the System}

The system queries a multimedia object by first extracting the features from the queried subjects, after which the system could procure a possiblity vector by feeding the feature data into the machine learning model. The machine learning model is trained by the score vectors and features from the objects already in the database. Score vectors are given by administrator or users. Thus we can conclude that there are several factors that could affect the performance (either goodness or effectiveness) of the system:
\begin{description}
\item[Score vectors] A score vector is a vector of 8 floating-point numbers whose value lies in the range of $[0.0, 1.0]$. The eight dimensions of the score vector could affect the system in a way that the dimensions don't covers enough possible human emotions (goodness is harmed), or that they are not orthogonal so that the effectiveness is harmed. The score vector could also be affected by subjective scoring if there are few users choosing to score a clip, in which case goodness is harmed again.
\item[Feature extraction] The features extracted from the clips could be not so representative that the features extracted are only partially useful and not all the aspects of the clips are considered. In this case, the goodness of the system is harmed. Or that too many features are extracted so that it takes too long to process a single clip, which slows down the performance of the system.
\item[Database population] If there are too few clips in the database, or that they have roughly the same score vector so that the machine learning module is not trained enough, the query result could be less satifying, i.e. goodness is harmed.
\item[Machine Learning] The parameters of the Convoluted Neural Network we used could have a great impact on the prediction of the score vectors of the queried subjects.
\end{description}

\subsection{Effectiveness}

From the perspective of effectiveness, the performance of the system will be measured by how fast the results can be fetched. To be more precise,

\begin{itemize}
\item How fast the required features are extracted from source inputs.
\item How fast the machine learning module outputs its predicted scores.
\item How fast the database is able to find out all the objects (or as
  many as the users like) with similar scores.
\end{itemize}

However, it's worth pointing out that, although we are measuring the effectiveness, we don't expect our system to be faster than other softwares of the same category. We are measuring the speed of the queries because we want to make sure that the performance of our system is tolerable. Another reason for this is that there aren't many standards we can rely on for the particular kind of software we are building as far as we are concerned.

\subsubsection{Brief Evaluation of Effectiveness}
The previous execution speed of the system is not acceptable. The bottleneck seems to be feature extraction, where a lot of numerical and array computations are done in plain Python. It is worth pointing out that because the numbers in Python is of infinite size, it's really inefficient to work with a lot of them. The standard Python iterable object, i.e. list, is of the same reason so that it fails to perform well under such heavy load of computation: it is dynamically allocated, where in our use case, we could allocate enough amount of memory for the feature before the computation.

After changing our code for feature extraction to Cython, the execution speed for acoustic feature extraction gains a $50\%$ speed-up, while the speed for visual feature extraction is still the same. The reason might be that the workload of visual feature extraction is at OpenCV, rather than the outer code that invokes OpenCV.

\subsection{Goodness}

From the perspective of goodness, however, it won’t be so simple that computers can do, since whether some audio fits some other video (or vice versa) is a very subjective matter. So we plan to evaluate the goodness of the retrieval manually, i.e., we will prepare a dataset of 20 (videos and audios respectively) to populate the database, another two datasets of the same size (one with only audios and one with only videos) to be queried separately and manually to tell the goodness of the results. The datasets will mainly come from the music rhythm video game \textit{Beatmania IIDX}, in which all the music are paired with a video clip intended as the visualization of the song. Some of the video clips are unique to certain songs, some are universal.

When testing the goodness of the system, we will find volunteers to help evaluate the goodness of the results.

\subsubsection{Brief Evaluation of Goodness}

In overall, it seems acceptable in that if one queries an audio clip (which is in the database), it replies with a series of video clips, where the video clip respect to the audio clip is of the least distance. This may be that the features and the predicted classification is working, because the scores of these two clips are alike.

However, due to the currently small size of dataset, the machine learning model does not usually give appropriate result (evaluated by the judgement from ourselves) for query subjects other than the ones already in the database. We will improve our ML model and experiment on other ML models to achieve the best goodness.

\bibliographystyle{plain}
\nocite{*}
\bibliography{bib}

\end{document}


